# -*- coding: utf-8 -*-
"""ID/X_Project_MuhamadRajwaAthoriq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ldh3PU9jJXoLYr3ndHyAydvBf5yHJi-s

# **Final Project - Project Based Internship - ID/X Partners**
---

> *Backgroud Problem:*

Kamu akan berkolaborasi dengan berbagai departemen lain dalam projek ini untuk menyediakan solusi teknologi bagi company tersebut. Kamu diminta untuk membangun model yang dapat memprediksi credit risk menggunakan dataset yang disediakan oleh company yang terdiri dari data pinjaman yang diterima dan yang ditolak. Selain itu kamu juga perlu mempersiapkan media visual untuk mempresentasikan solusi ke klien. Pastikan media visual yang kamu buat jelas, mudah dibaca, dan komunikatif. Pengerjaan end-to-end solution ini dapat dilakukan di Programming Language pilihanmu dengan tetap mengacu kepada framework/methodology Data Science.

## Importing Library
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import warnings
warnings.filterwarnings("ignore")

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

"""## Load Dataset"""

df = pd.read_csv('/content/loan_data_2007_2014.csv')
df.drop(df.columns[0], axis=1, inplace=True)
df.head()

"""## Data Understanding"""

ListofDecs = []

for i in df.columns:
  ListofDecs.append([i, df[i].dtype, df.shape[0], df[i].isna().sum(), round((df[i].isna().sum()/df.shape[0])*100, 2), df[i].nunique(), df[i].unique()[:5]])

df_decs = pd.DataFrame(ListofDecs, columns=['column', 'type', 'total row', 'null', 'percentage null', 'number of unique value', 'sample unique value'])
df_decs

"""**Kesimpulan**  

Berdasarkan hasil deskripsi setiap kolom, dapat disimpulkan bahwa dataset ini terdiri dari **74 kolom** dan **466.285 baris data**.  

Untuk meningkatkan kualitas data dan memastikan hanya fitur yang relevan digunakan dalam analisis lebih lanjut, akan dilakukan langkah-langkah berikut:  

1. **Penghapusan Kolom dengan Nilai Null Lebih dari 60%**  
   Kolom-kolom berikut memiliki lebih dari 60% data yang bernilai null sehingga dianggap kurang informatif dan akan dihapus:  
   
   - `desc`  
   - `mths_since_last_record`  
   - `annual_inc_joint`  
   - `dti_joint`  
   - `verification_status_joint`  
   - `open_acc_6m`  
   - `open_il_6m`  
   - `open_il_12m`  
   - `open_il_24m`  
   - `mths_since_rcnt_il`  
   - `total_bal_il`  
   - `il_util`  
   - `open_rv_12m`  
   - `open_rv_24m`  
   - `max_bal_bc`  
   - `all_util`  
   - `inq_fi`  
   - `total_cu_tl`  
   - `inq_last_12m`
   - `mths_since_last_major_derog`
   - `next_pymnt_d`
   - `mths_since_last_delinq`

2. **Penghapusan Kolom yang Kurang Relevan**  
   Selain itu, beberapa kolom yang dianggap tidak memberikan kontribusi signifikan terhadap analisis juga akan dihapus, yaitu:  

   - `id`  
   - `member_id`  
   - `emp_title`  
   - `url`  
   - `desc`  
   - `policy_code`  
   - `application_type`
   - `zip_code`  
   - `inq_last_6mths`
   - `delinq_2yrs`

Langkah ini bertujuan untuk menyederhanakan dataset dan meningkatkan efisiensi dalam proses analisis data. Hanya fitur yang relevan dan berpotensi memberikan wawasan penting yang akan dipertahankan.  

"""

# mengubah loan_status
df['loan_status'] = df['loan_status'].apply(lambda x: "good" if x in ['Current', 'Fully Paid'] else 'bad')

df = df.drop([
    'desc', 'mths_since_last_record', 'annual_inc_joint', 'dti_joint',
    'verification_status_joint', 'open_acc_6m', 'open_il_6m',
    'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il',
    'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util',
    'inq_fi', 'total_cu_tl', 'inq_last_12m', 'member_id',
    'emp_title', 'url', 'policy_code', 'application_type', 'mths_since_last_delinq',
    'zip_code', 'pymnt_plan', 'mths_since_last_major_derog', 'next_pymnt_d'
], axis=1)

cat = df.select_dtypes('object').columns
num = df.select_dtypes('number').columns

print(f'Number of Numeric Columns: {len(num)}')
print(f'Number of Categoric Columns: {len(cat)}')

# Cari nilai yang mengandung simbol $
for col in df.columns:
    if df[col].astype(str).str.contains(r'\$', regex=True).any():
        print(f"Kolom '{col}' memiliki nilai dengan simbol '$'")

# Bersihkan simbol $ dalam semua kolom string
df = df.replace(r'\$', '', regex=True)

"""### Univariate Analysis"""

cat_plot = ['term', 'emp_length', 'home_ownership', 'verification_status', 'loan_status', 'purpose', 'initial_list_status']

plt.figure(figsize=(20,8))

plt.style.use('ggplot')

for i, col in enumerate(cat_plot):
  plt.subplot(2, 4, i+1)
  sns.countplot(data=df, x=col, color='blue')
  plt.title(col, fontsize=15, weight='bold', usetex=False)
  plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

"""**Kesimpulan**

1. Trem
> Nasabah terbagi dalam dua tenor pinjaman, yakni 33 ribu untuk 36 bulan dan 13 ribu untuk 60 bulan, dengan mayoritas memilih tenor lebih pendek.
2. emp_length
> Mayoritas nasabah yang mengajukan pinjaman ialah nasabah yang sudah bekerja lebih dari 10 tahun
3. home_ownership
> Mayoritas peminjam memiliki status kepemilikan rumah MORTGAGE (hipotek) atau RENT (menyewa). Sementara itu, nasabah dengan rumah sendiri (OWN) lebih sedikit, dan kategori lain hampir tidak ada, menunjukkan banyak peminjam masih mencicil rumah atau menyewa.
4. verification_status
> Mayoritas nasabah memiliki status verifikasi Verified, diikuti oleh Source Verified dan Not Verified dengan jumlah yang hampir sama.
5. loan_status
> Mayoritas pinjaman Current dan Fully Paid, sementara sedikit yang gagal bayar atau terlambat, menunjukkan risiko kredit rendah.
6. Purpose
> Mayoritas nasabah mengajukan pinjaman untuk melunasi utang lama (debt consolidation), disusul oleh kebutuhan kartu kredit (credit card).
7. initial_list_status
> Mayoritas peminjam memiliki status Fractional (f) mencapai 30rb dan berstatus Whole (w) mencapai 16rb
"""

plt.figure(figsize=(30,30))

plt.style.use('ggplot')

for i, col in enumerate(num):
  plt.subplot(8, 4, i+1)
  sns.histplot(data=df, x=col, color='blue', kde=True)
  plt.title(col, fontsize=15, weight='bold', usetex=False)

plt.tight_layout()
plt.show()

plt.figure(figsize=(30,30))

plt.style.use('ggplot')

for i, col in enumerate(num):
  plt.subplot(8, 4, i+1)
  sns.boxplot(data=df, x=col, color='blue')
  plt.title(col, fontsize=15, weight='bold', usetex=False)

plt.tight_layout()
plt.show()

# heatmap
plt.figure(figsize=(20,20))
sns.heatmap(df[num].corr(), annot=True, fmt='.2f')
plt.tight_layout()
plt.show()

"""**Kesimpulan**

Terdapat beberapa kolom feature yang redundant diantaranya:
1. ``loan_amnt`` dengan ``funded_amnt``
2. ``funded_amnt_ivn`` dengan ``installment``
3. ``total_pymnt`` dengan ``total_pymnt_ivn``
4. ``last_pymnt_amnt`` dengan ``total_rec_int``
5......

Akan dilakukan penghapusan terhadap salah satu dari kedua kolom tersebut agar menghindari **Multikolinearitas**

### Multivariate Analysis
"""

def distriburion_percentage_by_category(col):
  df_dist_perce = df.groupby([col, 'loan_status']).agg(Count=('loan_status', 'count')).reset_index()
  df_dist_perce['Percentage'] = ((df_dist_perce['Count'] / df_dist_perce.groupby(col)['Count'].transform('sum')) * 100).round(2)
  df_dist_perce.sort_values('Percentage', ascending= False, inplace=True)
  return df_dist_perce

def plot_distribution_percentage_category(col, orient):
  # menyiapkan data
  data = distriburion_percentage_by_category(col)

  if orient == 'h':
    plt.figure(figsize=(12,8))
    ax = sns.barplot(y=col, x='Percentage', hue='loan_status', data=data, orient=orient, palette="Blues_d")
    for p in ax.patches:
        width = p.get_width()
        ax.annotate(f'{width:.2f}%',
                    xy=(width / 2, p.get_y() + p.get_height() / 2),
                    ha='center', va='center', color='white', fontsize=10, fontweight='bold')

    plt.xlabel('Percentage by Total Clients', fontsize=12)
    plt.ylabel(f'Loan {col}', fontsize=12)

  else:
    plt.figure(figsize=(12,8))
    ax =sns.barplot(data=data, x=col, y='Percentage', hue='loan_status', palette='Blues_d')
    for p in ax.patches:
      height = p.get_height()
      ax.annotate(f'{height:.2f}%',
                  xy=(p.get_x() + p.get_width() / 2, height/2),
                  ha='center', va='bottom', fontsize=10, color='white', fontweight='bold')

    plt.ylabel('Percentage by Total Clients', fontsize=12)
    plt.xlabel(f'Loan {col}', fontsize=12)

  plt.title(f"Percentage of Overall {col} distribution", fontsize=14, fontweight='bold')
  plt.legend(title='Loan Status', fontsize=10, title_fontsize=12)
  plt.tight_layout()
  plt.show()

"""##### **Term**"""

plot_distribution_percentage_category('term', 'v')

"""#### **Purpose**"""

plot_distribution_percentage_category('purpose', 'h')

"""####**Grade**"""

plot_distribution_percentage_category('grade', 'h')

"""#### **Verification Status**"""

plot_distribution_percentage_category('verification_status', 'v')

"""#### **emp_length**"""

plot_distribution_percentage_category('emp_length', 'h')

"""#### **Initial List Status**"""

plot_distribution_percentage_category('initial_list_status', 'v')

"""#### **Home Ownership**"""

plot_distribution_percentage_category('home_ownership', 'h')

"""#### **Annual Income**"""

plt.figure(figsize=(10,6))
sns.histplot(data=df, x='annual_inc', hue='loan_status', kde=True)

# Hitung rata-rata
mean_annual_inc = df['annual_inc'].mean()
mean_good_annual_inc = df[df['loan_status'] == 'good']['annual_inc'].mean()
mean_bad_annual_inc = df[df['loan_status'] == 'bad']['annual_inc'].mean()

# Tambahkan garis vertikal
plt.axvline(mean_annual_inc, color='#28B463', linestyle='dashed', label=f'Rata-rata Keseluruhan: {mean_annual_inc:.2f}')
plt.axvline(mean_good_annual_inc, color='#2E86C1', linestyle='dotted', label=f'Rata-rata Good Loan: {mean_good_annual_inc:.2f}')
plt.axvline(mean_bad_annual_inc, color='#FF5733', linestyle='solid', label=f'Rata-rata Bad Loan: {mean_bad_annual_inc:.2f}')

# Tambahkan teks pada grafik
plt.text(mean_annual_inc, plt.ylim()[1]*0.9, f'Rata-rata Keseluruhan\n{mean_annual_inc:.2f}', color='#28B463', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))
plt.text(mean_good_annual_inc, plt.ylim()[1]*0.8, f'Rata-rata Good Loan\n{mean_good_annual_inc:.2f}', color='#2E86C1', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))
plt.text(mean_bad_annual_inc, plt.ylim()[1]*0.7, f'Rata-rata Bad Loan\n{mean_bad_annual_inc:.2f}', color='#FF5733', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))

plt.title('Distribusi Annual Income Berdasarkan Status Pinjaman', fontsize=14)
plt.xlabel('Annual Income (annual_inc)', fontsize=12)
plt.ylabel('Jumlah Nasabah', fontsize=12)
plt.tight_layout()
plt.show()

df['annual_inc_categories'] = df['annual_inc'].apply(lambda x: 'Low Income' if x <= 50000 else ('Middle Income' if x <=100000 else 'High Income'))
sns.countplot(data=df, x='annual_inc_categories', color='blue', alpha=.7)
plt.title('Distribution Categories Annual Income')

plot_distribution_percentage_category('annual_inc_categories', 'v')

"""#### **Debt to Income Ratio**"""

plt.figure(figsize=(10,6))
sns.histplot(data=df, x='dti', hue='loan_status', kde=True)

# Hitung rata-rata
mean_dti = df['dti'].mean()
mean_good_dti = df[df['loan_status'] == 'good']['dti'].mean()
mean_bad_dti = df[df['loan_status'] == 'bad']['dti'].mean()

# Tambahkan garis vertikal
plt.axvline(mean_dti, color='#28B463', linestyle='dashed', label=f'Rata-rata Keseluruhan: {mean_dti:.2f}')
plt.axvline(mean_good_dti, color='#2E86C1', linestyle='dotted', label=f'Rata-rata Good Loan: {mean_good_dti:.2f}')
plt.axvline(mean_bad_dti, color='#FF5733', linestyle='solid', label=f'Rata-rata Bad Loan: {mean_bad_dti:.2f}')

# Tambahkan teks pada grafik
plt.text(mean_dti, plt.ylim()[1]*0.9, f'Rata-rata Keseluruhan\n{mean_dti:.2f}', color='#28B463', ha='right', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))
plt.text(mean_good_dti, plt.ylim()[1]*0.8, f'Rata-rata Good Loan\n{mean_good_dti:.2f}', color='#2E86C1', ha='right', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))
plt.text(mean_bad_dti, plt.ylim()[1]*0.7, f'Rata-rata Bad Loan\n{mean_bad_dti:.2f}', color='#FF5733', ha='right', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))

plt.title('Distribusi Debt-to-Income Ratio Berdasarkan Status Pinjaman', fontsize=14)
plt.xlabel('Debt-to-Income Ratio (DTI)', fontsize=12)
plt.ylabel('Jumlah Nasabah', fontsize=12)
plt.tight_layout()
plt.show()

df['dti_categories'] = df['dti'].apply(lambda x: 'Low DTI' if x <= 20 else ('Middle DTI' if x <=35 else 'High DTI'))
sns.countplot(data=df, x='dti_categories', color='blue', alpha=.7)
plt.title('Distribution Categories Debt-to-Income Ratio')

plot_distribution_percentage_category('dti_categories', 'v')

"""## Data Preprocessing"""

upper_tri = df[num].corr().where(np.triu(np.ones(df[num].corr().shape, dtype=bool), k=1))

col_to_drop = [col for col in upper_tri.columns if any(upper_tri[col] >.9)]
print(col_to_drop)

col_num_selected = num.drop(col_to_drop).to_list()
print(col_num_selected)

col_cat_selected = ['term', 'purpose', 'grade', 'sub_grade', 'verification_status', 'emp_length', 'initial_list_status', 'home_ownership', 'loan_status', 'earliest_cr_line']
print(col_cat_selected)

print(f'Jumlah kolom Numerik yang didrop: {len(col_to_drop)}')
print(f'Jumlah kolom Numerik: {len(col_num_selected)}')
print(f'Jumlah kolom Kategori: {len(col_cat_selected)}')

col_selected = col_cat_selected + col_num_selected

df_prep = df[col_selected]

"""### Handle Missing Values"""

df_prep.isnull().sum()

def impute_missing_values(data):
  for col in data.columns:
    if pd.api.types.is_numeric_dtype(data[col]):
      data[col].fillna(data[col].median(), inplace=True)
    else:
      data[col].fillna(data[col].mode()[0], inplace=True)

  return data

df_prep = impute_missing_values(df_prep)
df_prep.isnull().sum()

"""### Handle Duplicated Values"""

(f'Jumlah Data Duplikat: {df_prep.duplicated().sum()}')

"""### Feature Engineering"""

# Mengonversi kolom 'earliest_cr_line' menjadi format datetime
df_prep['earliest_cr_line'] = pd.to_datetime(df_prep['earliest_cr_line'], format='%b-%y')
df_prep['earliest_cr_line_year'] = df_prep['earliest_cr_line'].dt.year.clip(upper=2014)

def credit_history_conversion(df):
    for index, rows in df.iterrows():
        credit_history = 2014 - rows['earliest_cr_line_year']
        if credit_history <= 2:
            df.at[index, 'credit_history'] = '0-2 years'
        elif credit_history <= 5:
            df.at[index, 'credit_history'] = '2-5 years'
        else:
            df.at[index, 'credit_history'] = '+5 years'

credit_history_conversion(df_prep)

df_prep = df_prep.drop(columns=['earliest_cr_line', 'earliest_cr_line_year'])

"""### Encoding Values"""

df_prep.head()

from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# Label Encoding
df_prep['term'] = LabelEncoder().fit_transform(df_prep['term'])
df_prep['sub_grade'] = LabelEncoder().fit_transform(df_prep['sub_grade'])
df_prep['credit_history'] = LabelEncoder().fit_transform(df_prep['credit_history'])
df_prep['initial_list_status'] = df['initial_list_status'].map({'f': 0, 'w':1})
df_prep['grade'] =df_prep['grade'].map({'A':0, 'B':1, 'C':2, 'D':3, 'E':4, 'F':5, 'G':6})
df_prep['loan_status'] = df_prep['loan_status'].map({'bad':0, 'good':1})

# One Hot Encoding
to_one_hot = ['purpose', 'verification_status', 'emp_length', 'home_ownership']

one_hot_encoder = OneHotEncoder(drop='first', handle_unknown='ignore')

one_hot_df = one_hot_encoder.fit_transform(df_prep[to_one_hot])
one_hot_df = pd.DataFrame(one_hot_df.toarray(), columns=one_hot_encoder.get_feature_names_out(to_one_hot))

df_prep = df_prep.drop(columns=to_one_hot).join(one_hot_df)

df_prep.head()

"""### Transformation Values"""

for col in col_num_selected:
  df_prep[col] = np.log1p(df_prep[col])

df_prep.head()

"""## Modeling"""

from sklearn.model_selection import train_test_split

X = df_prep.drop('loan_status', axis=1)
y = df_prep['loan_status']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)

print(f'Shape of X_train: {X_train.shape}')
print(f'Shape of X_test: {X_test.shape}')

"""### SMOTE"""

print(f'Number of Good Loan Before SMOTE: {y_train.value_counts()[1]}')
print(f'Number of Bad Loan Before SMOTE: {y_train.value_counts()[0]}')

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_smt, y_train_smt = smote.fit_resample(X_train, y_train)

print(f'Number of Good Loan After SMOTE: {y_train_smt.value_counts()[1]}')
print(f'Number of Bad Loan After SMOTE: {y_train_smt.value_counts()[0]}')

"""### Train Model"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

def train_model(X_train, y_train, X_test, y_test):
  # list model
  models ={
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Ada Boost': AdaBoostClassifier(random_state=42)
    }


  result = []
  for nama_model, model in models.items():
    model.fit(X_train, y_train)
    y_pred_test = model.predict(X_test)
    y_pred_train = model.predict(X_train)


    # data train
    acc_train = round(accuracy_score(y_train, y_pred_train),2)
    prec_train = round(precision_score(y_train, y_pred_train),2)
    rec_train = round(recall_score(y_train, y_pred_train),2)
    f1_train = round(f1_score(y_train, y_pred_train),2)
    roc_train = round(roc_auc_score(y_train, y_pred_train),2)

    # data test
    acc_test = round(accuracy_score(y_test, y_pred_test),2)
    prec_test = round(precision_score(y_test, y_pred_test),2)
    rec_test = round(recall_score(y_test, y_pred_test),2)
    f1_test = round(f1_score(y_test, y_pred_test),2)
    roc_test = round(roc_auc_score(y_test, y_pred_test),2)

    # result
    result.append([nama_model, acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
  result_df = pd.DataFrame(result, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
  return result_df

result = train_model(X_train_smt, y_train_smt, X_test, y_test)
print('With SMOTE')
result

result = train_model(X_train, y_train, X_test, y_test)
print('Without SMOTE')
result

model = LogisticRegression(random_state=42)
model.fit(X_train_smt, y_train_smt)

result_default = []

y_pred_test = model.predict(X_test)
y_pred_train = model.predict(X_train_smt)

# data train
acc_train = round(accuracy_score(y_train_smt, y_pred_train),2)
prec_train = round(precision_score(y_train_smt, y_pred_train),2)
rec_train = round(recall_score(y_train_smt, y_pred_train),2)
f1_train = round(f1_score(y_train_smt, y_pred_train),2)
roc_train = round(roc_auc_score(y_train_smt, y_pred_train),2)

# data test
acc_test = round(accuracy_score(y_test, y_pred_test),2)
prec_test = round(precision_score(y_test, y_pred_test),2)
rec_test = round(recall_score(y_test, y_pred_test),2)
f1_test = round(f1_score(y_test, y_pred_test),2)
roc_test = round(roc_auc_score(y_test, y_pred_test),2)

# showing confusion metrics
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix AdaBoostClassifier(Default)')
plt.xlabel('Predicted')
plt.ylabel('Actual')

result_default.append(['LogisticRegression(Default)', acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
result_default = pd.DataFrame(result_default, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
result_default

"""### Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2', 'elasticnet', None],
    'solver': ['liblinear', 'lbfgs', 'saga'],
    'max_iter': [100, 200, 500, 1000],
}

# Initialize Logistic Regression
adaboost = LogisticRegression(random_state=42)

# Initialize GridSearchCV
random_search = RandomizedSearchCV(
    estimator=adaboost,
    param_distributions=param_grid,
    n_iter=10,               # Jumlah iterasi (kombinasi) untuk dicoba
    cv=3,                    # Cross-validation 5 fold
    scoring='roc_auc',       # Metode evaluasi ROC AUC
    n_jobs=-1,               # Paralel untuk mempercepat
    random_state=42          # Seed untuk hasil yang konsisten
)

# Fit the grid search to the data
random_search.fit(X_train_smt, y_train_smt)

# Get the best parameters and best score
best_params = random_search.best_params_

print(f"Best parameters: {best_params}")

# Train the model with the best parameters
model_tuning = random_search.best_estimator_
model_tuning.fit(X_train_smt, y_train_smt)

# Evaluate the model (example using roc_auc_score)
result_tuning = []

y_pred_test = model_tuning.predict(X_test)
y_pred_train = model_tuning.predict(X_train_smt)

# data train
acc_train = round(accuracy_score(y_train_smt, y_pred_train),2)
prec_train = round(precision_score(y_train_smt, y_pred_train),2)
rec_train = round(recall_score(y_train_smt, y_pred_train),2)
f1_train = round(f1_score(y_train_smt, y_pred_train),2)
roc_train = round(roc_auc_score(y_train_smt, y_pred_train),2)

# data test
acc_test = round(accuracy_score(y_test, y_pred_test),2)
prec_test = round(precision_score(y_test, y_pred_test),2)
rec_test = round(recall_score(y_test, y_pred_test),2)
f1_test = round(f1_score(y_test, y_pred_test),2)
roc_test = round(roc_auc_score(y_test, y_pred_test),2)

# showing confusion metrics
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix AdaBoostClassifier (Tuning)')
plt.xlabel('Predicted')
plt.ylabel('Actual')

result_tuning.append(['LogisticRegression(Tuning)', acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
result_tuning = pd.DataFrame(result_tuning, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
result_tuning

full_result = pd.concat([result_default, result_tuning], axis=0)
full_result

"""### Feature Importance"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Mengambil nilai koefisien dari model
importances = model_tuning.coef_[0]
feature_names = X_train_smt.columns

# Membuat DataFrame untuk visualisasi
feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})

# Mengurutkan berdasarkan nilai absolut dari importance dan mengambil 10 fitur teratas
top_10_features = feature_importance_df.reindex(feature_importance_df['Importance'].abs().nlargest(10).index)

# Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=top_10_features, palette='viridis')
plt.title('Top 10 Feature Importance (Logistic Regression)')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()